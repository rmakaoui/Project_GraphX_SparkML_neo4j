{
  "metadata": {
    "name": "Manipulation\u0026Insertion",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark \n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, trim, lower\n\n# Create a Spark session\nspark \u003d SparkSession.builder.appName(\"DataCleaning\").config(\"spark.driver.extraClassPath\", \"path/to/your/hadoop/home/lib/*\").getOrCreate()\n\n# Assuming book1, book2, book3, book4, and book5 are PySpark DataFrames\n# Replace the placeholders with your actual data or loading mechanisms.\n\n# Load or create your DataFrames\nbook1 \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/book1.csv\", header\u003dTrue)\nbook2 \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/book2.csv\", header\u003dTrue)\nbook3 \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/book3.csv\", header\u003dTrue)\nbook4 \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/book4.csv\", header\u003dTrue)\nbook5 \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/book5.csv\", header\u003dTrue)\n\n# Concatenate the DataFrames vertically\nconcatenated_df \u003d book1.union(book2).union(book3).union(book4).union(book5)\n\n# Display the schema and the first few rows of the concatenated DataFrame\nconcatenated_df.printSchema()\nconcatenated_df.show()\n\n# Data Cleaning Steps\n\n# 1. Remove leading and trailing whitespaces from string columns\nconcatenated_df \u003d concatenated_df.withColumn(\"Source\", trim(col(\"Source\").cast(\"string\")))\nconcatenated_df \u003d concatenated_df.withColumn(\"Target\", trim(col(\"Target\").cast(\"string\")))\n\n# 2. Handle outliers or incorrect values in the \"weight\" column\n# For example, filter out rows where weight is negative\nconcatenated_df \u003d concatenated_df.filter(col(\"weight\") \u003e\u003d 0)\n\n# 3. Standardize or clean up categorical values\n# For example, convert all names to lowercase for consistency\nconcatenated_df \u003d concatenated_df.withColumn(\"Source\", lower(col(\"Source\").cast(\"string\")))\nconcatenated_df \u003d concatenated_df.withColumn(\"Target\", lower(col(\"Target\").cast(\"string\")))\nconcatenated_df \u003d concatenated_df.na.drop(subset\u003d[\"Source\", \"Target\", \"weight\"])\nconcatenated_df \u003d concatenated_df.filter(col(\"Source\").isNotNull() \u0026 col(\"Target\").isNotNull())\n\n# 4. Drop unnecessary columns\n# If there are columns that are not needed for analysis, drop them\nconcatenated_df.show(5)\nconcatenated_df \u003d concatenated_df.na.drop()\nconcatenated_df.printSchema()\n\n# Save the cleaned and concatenated DataFrame to a single CSV file\nconcatenated_df.coalesce(1).write.mode(\"overwrite\").csv(\"/zeppelin/notebook/resultUnion.csv\", header\u003dTrue)\n# Load the cleaned DataFrame\ncleaned_df \u003d spark.read.format(\"csv\").load(\"/zeppelin/notebook/resultUnion.csv\", header\u003dTrue)\ncleaned_df.show(10)\n# Load the cleaned DataFrame"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.neo4j.driver._\nimport org.neo4j.driver.Values\nimport scala.io.Source\n\nval uri \u003d \"bolt://neo4j:7687\"\nval user \u003d \"neo4j\"\nval password \u003d \"bitnami1\"\n\nval driver \u003d GraphDatabase.driver(uri, AuthTokens.basic(user, password))\nval session \u003d driver.session()\n\n// Use HDFS file path\nval csvFile \u003d \"/zeppelin/notebook/resultUnion.csv/part-00000-fa647f61-612d-40ed-81b5-1c8049caa127-c000.csv\"\nval lines \u003d Source.fromFile(csvFile).getLines().toList\n\n// Extract header and data\nval header \u003d lines.head.split(\",\")\nval data \u003d lines.tail.map(_.split(\",\"))\n\n// Insert data into Neo4j with a different graph structure\ndata.foreach { row \u003d\u003e\n  // Check if the row has enough elements\n  if (row.length \u003e\u003d 5) {\n    val cypherQuery \u003d\n      s\"\"\"\n         |MERGE (source:Node1 {name: \u0027${row(0)}\u0027})\n         |MERGE (target:Node1 {name: \u0027${row(1)}\u0027})\n         |MERGE (source)-[r:${row(2)} {weight: ${row(3).toInt}, book: ${row(4).toInt}}]-\u003e(target)\n       \"\"\".stripMargin\n\n    session.run(cypherQuery)\n  } else {\n    println(s\"Skipping row: ${row.mkString(\", \")} - Insufficient columns\")\n  }\n}\n\n// Close the session and driver when done\nsession.close()\ndriver.close()"
    }
  ]
}