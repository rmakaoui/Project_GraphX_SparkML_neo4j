{
  "metadata": {
    "name": "CRUD",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\r\nMATCH (n:Node1)\r\nRETURN n;"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.{SaveMode, SparkSession}\n\nval spark \u003d SparkSession.builder().getOrCreate()\n\nval df1 \u003d (spark.read.format(\"org.neo4j.spark.DataSource\")\n  .option(\"url\", \"bolt://neo4j:7687\")\n  .option(\"authentication.basic.username\", \"neo4j\")\n  .option(\"authentication.basic.password\", \"bitnami1\")\n  .option(\"labels\", \"Node1\")\n  .load())\n\ndf1.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//CREATION DU GRAPH\r\n\r\nimport org.apache.spark.graphx._\r\nimport org.apache.spark.rdd.RDD\r\n\r\n// Extract vertices from DataFrame\r\nval vertices: RDD[(VertexId, String)] \u003d df1\r\n  .select(\"name\")  // Replace \"id\" with the actual column name for vertex IDs\r\n  .distinct()\r\n  .rdd\r\n  .map(row \u003d\u003e (row.getLong(0), row.getString(1)))  // Adjust column indices and types\r\n\r\n// Extract edges from DataFrame\r\nval edges: RDD[Edge[String]] \u003d df1\r\n  .select(\"name\")  // Replace with actual column names\r\n  .rdd\r\n  .map(row \u003d\u003e Edge(row.getLong(0), row.getLong(1), row.getString(2)))  // Adjust column indices and types\r\n\r\n// Create the GraphX graph\r\nval graph \u003d Graph(vertices, edges)\r\n\r\n// Now you can apply GraphX algorithms on \u0027graph\u0027\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\r\n\r\n\r\n\r\nMATCH (n:Node1) WITH n MATCH (n)-[relation]-\u003e(target) \r\nWHERE relation.book IS NOT NULL AND relation.weight IS NOT NULL \r\nRETURN n.name AS Source, target.name AS Target, TYPE(relation) AS Type, relation.weight AS Weight, relation.book AS Book\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.{SaveMode, SparkSession}\r\n\r\n// Configuration pour la connexion à Neo4j\r\nval neo4jUrl \u003d \"bolt://neo4j:7687\"\r\nval neo4jUser \u003d \"neo4j\"\r\nval neo4jPassword \u003d \"bitnami1\"\r\n\r\nval spark \u003d SparkSession.builder().getOrCreate()\r\n\r\nval all \u003d (spark.read.format(\"org.neo4j.spark.DataSource\")\r\n  .option(\"url\", \"bolt://neo4j:7687\")\r\n  .option(\"authentication.basic.username\", neo4jUser)\r\n  .option(\"authentication.basic.password\", neo4jPassword)\r\n  .option(\"query\", \"MATCH (n:Node1) WITH n MATCH (n)-[relation]-\u003e(target)  WHERE relation.book IS NOT NULL AND relation.weight IS NOT NULL RETURN n.name AS Source, target.name AS Target, TYPE(relation) AS Type, relation.weight AS Weight, relation.book AS Book\")\r\n  .load())\r\n all.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Filter sources with book \u003d 1\r\nval sourcesWithBook1 \u003d all.filter(\"Book \u003d 1\").select(\"Source\", \"Book\").distinct()\r\n\r\n// Show the filtered sources\r\nsourcesWithBook1.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Filter sources with book \u003d 1\r\nval sourcesWithBook1 \u003d all.filter(\"Book \u003d 2\").select(\"Source\", \"Book\").distinct()\r\n\r\n// Show the filtered sources\r\nsourcesWithBook1.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Filter sources with book \u003d 1\r\nval sourcesWithBook1 \u003d all.filter(\"Book \u003d 3\").select(\"Source\", \"Book\").distinct()\r\n\r\n// Show the filtered sources\r\nsourcesWithBook1.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Filter sources with book \u003d 1\r\nval sourcesWithBook1 \u003d all.filter(\"Book \u003d 4\").select(\"Source\", \"Book\").distinct()\r\n\r\n// Show the filtered sources\r\nsourcesWithBook1.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Filter sources with book \u003d 1\r\nval sourcesWithBook1 \u003d all.filter(\"Book \u003d 5\").select(\"Source\", \"Book\").distinct()\r\n\r\n// Show the filtered sources\r\nsourcesWithBook1.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\r\n\r\n// Calculate the total number of books\r\nval totalNumberOfBooks \u003d all.select(\"Book\").distinct().count()\r\n\r\n// Group by Source and count distinct books\r\nval sourceBookCounts \u003d all.groupBy(\"Source\").agg(countDistinct(\"Book\").alias(\"DistinctBooks\"))\r\n\r\n// Filter sources that exist in all books\r\nval sourcesInAllBooks \u003d sourceBookCounts.filter(col(\"DistinctBooks\") \u003d\u003d\u003d lit(totalNumberOfBooks)).select(\"Source\")\r\n\r\n// Show the result\r\nsourcesInAllBooks.show()\r\nval numberOfSourcesInAllBooks \u003d sourcesInAllBooks.count()\r\nval numberOfAllSources \u003d all.select(\"Source\").distinct().count()\r\n// Print the result\r\nprintln(s\"The number of actors that exist in all books is: $numberOfSourcesInAllBooks out of $numberOfAllSources\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\r\n\r\n// Group by Target and count occurrences\r\nval targetOccurrences \u003d all.groupBy(\"Target\").agg(count(\"*\").alias(\"Occurrences\"))\r\n\r\n// Find the target with the most occurrences\r\nval mostFrequentTarget \u003d targetOccurrences.orderBy(desc(\"Occurrences\")).select(\"Target\").first().getString(0)\r\n\r\n// Print the result\r\nprintln(s\"The target with the most occurrences is: $mostFrequentTarget\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.{SparkSession, Row}\r\nimport org.apache.spark.sql.types.StructType\r\n\r\nval spark \u003d SparkSession.builder().getOrCreate()\r\n\r\n// Your existing code to create the \u0027all\u0027 DataFrame\r\nval all \u003d (spark.read.format(\"org.neo4j.spark.DataSource\")\r\n  .option(\"url\", \"bolt://neo4j:7687\")\r\n  .option(\"authentication.basic.username\", \"neo4j\")\r\n  .option(\"authentication.basic.password\", \"bitnami1\")\r\n  .option(\"labels\", \"Node1\")\r\n  .load())\r\n\r\n// Retrieve one object (for example, the first row)\r\nval targetRow: Row \u003d all.collect()(0)\r\n\r\n// Modify the object (for example, update the \"Weight\" field)\r\nval modifiedRow \u003d Row.fromSeq(targetRow.toSeq.updated(all.schema.fieldIndex(\"name\"), \"reda-targarian\"))\r\n\r\n// Create a new schema with the same structure as \u0027all\u0027\r\nval modifiedSchema \u003d StructType(all.schema.fields)\r\n\r\n// Create a DataFrame with the modified row\r\nval modifiedDataFrame \u003d spark.createDataFrame(spark.sparkContext.parallelize(Seq(modifiedRow)), modifiedSchema)\r\n\r\n// Display the modified row\r\nprintln(\"Modified Row:\")\r\nmodifiedDataFrame.show()\r\n// Display the number of entities before insertion\r\nprintln(s\"Number of entities before insertion: ${all.count()}\")\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.{SaveMode, SparkSession}\r\nimport org.apache.spark.sql.functions._\r\n\r\nval spark \u003d SparkSession.builder().getOrCreate()\r\nimport spark.implicits._\r\n\r\n// Sample data\r\nval data \u003d Seq(\r\n  (\"Reda-targarian\", \"Jaime-Lannister\", \"Undirected\", 3, 1),\r\n  (\"oussama-targarian\", \"Tywin-Lannister\", \"Undirected\", 6, 1),\r\n  (\"elhadi-targarian\", \"Tywin-Lannister\", \"Undirected\", 6, 1)\r\n)\r\n\r\n// Define the schema for the DataFrame\r\nval schema \u003d Seq(\"Source\", \"Target\", \"Type\", \"Weight\", \"Book\")\r\n\r\n// Create a DataFrame\r\nval df \u003d spark.createDataFrame(data).toDF(schema: _*)\r\nprintln(s\"Number of entities before insertion: ${all.count()}\")\r\n// Save DataFrame to Neo4j\r\ndf.write\r\n  .format(\"org.neo4j.spark.DataSource\")\r\n  .mode(SaveMode.ErrorIfExists)\r\n  .option(\"url\", \"bolt://neo4j:7687\")\r\n  .option(\"authentication.basic.username\", \"neo4j\")\r\n  .option(\"authentication.basic.password\", \"bitnami1\")\r\n  .option(\"labels\", \":Node1\")\r\n  .option(\"source.labels\", \":Node1\")\r\n  .option(\"source.save.mode\", \"MATCH\")\r\n  .option(\"target.labels\", \":Node1\")\r\n  .option(\"target.save.mode\", \"MATCH\")\r\n  .save()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.{SparkSession, SaveMode}\r\n\r\nval spark \u003d SparkSession.builder().getOrCreate()\r\n\r\n// Read the data from Neo4j\r\nval neo4jData \u003d spark.read.format(\"org.neo4j.spark.DataSource\")\r\n  .option(\"url\", \"bolt://neo4j:7687\")\r\n  .option(\"authentication.basic.username\", \"neo4j\")\r\n  .option(\"authentication.basic.password\", \"bitnami1\")\r\n  .option(\"labels\", \":Node1\")\r\n  .load()\r\n\r\n// Compare with the original DataFrame\r\nprintln(\"Comparison with original data:\")\r\ndf.show()\r\n\r\n// Perform any necessary assertions or checks to verify the correctness\r\n// For example, you can compare the number of rows, schema, etc.\r\nif (neo4jData.count() \u003d\u003d df.count()) {\r\n  println(\"Data was successfully inserted into Neo4j.\")\r\n} else {\r\n  println(\"Data insertion into Neo4j might have issues.\")\r\n}\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\nMATCH (p:Node1 {Source: \u0027Reda-targarian\u0027})\nSET p.Source \u003d \u0027redaaa-targarian\u0027\nRETURN p"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\nMATCH (p:Node1 {Source: \u0027redaaa-targarian\u0027})\nRETURN p"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\nMATCH (n:Node1 {Source: \u0027redaaa-targarian\u0027})\nDELETE n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\nMATCH (p:Node1 {Source: \u0027redaaa-targarian\u0027})\nRETURN p"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%neo4j\nMATCH (p:Node1 {Source: \u0027oussama-targarian\u0027})\nRETURN p"
    }
  ]
}