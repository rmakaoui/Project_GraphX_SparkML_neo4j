{
  "metadata": {
    "name": "GraphX",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.{SaveMode, SparkSession}\n\n// Configuration pour la connexion à Neo4j\nval neo4jUrl \u003d \"bolt://neo4j:7687\"\nval neo4jUser \u003d \"neo4j\"\nval neo4jPassword \u003d \"bitnami1\"\n\nval spark \u003d SparkSession.builder().getOrCreate()\n\nval all \u003d (spark.read.format(\"org.neo4j.spark.DataSource\")\n  .option(\"url\", \"bolt://neo4j:7687\")\n  .option(\"authentication.basic.username\", neo4jUser)\n  .option(\"authentication.basic.password\", neo4jPassword)\n  .option(\"query\", \"MATCH (n:Node1) WITH n MATCH (n)-[relation]-\u003e(target) WHERE relation.book IS NOT NULL AND relation.weight IS NOT NULL RETURN n.name AS Source, target.name AS Target, TYPE(relation) AS Type, relation.weight AS Weight, relation.book AS Book;\")\n  .load())\n all.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nall.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//CREATION DU GRAPH\r\n\r\nimport org.apache.spark.sql.Row\r\nimport org.apache.spark.graphx.{Graph, Edge, VertexId}\r\nimport org.apache.spark.rdd.RDD\r\n\r\n// Let\u0027s create the vertex RDD.\r\nval vertices: RDD[(VertexId, String)] \u003d all\r\n  .selectExpr(\"explode(array(Target, Source)) as vertex\")\r\n  .distinct\r\n  .rdd\r\n  .map(_.getAs[String](0))\r\n  .zipWithIndex\r\n  .map(_.swap)\r\n\r\n// Now let\u0027s define a vertex dataframe because joins are clearer in Spark SQL\r\nval vertexDf \u003d vertices.toDF(\"id\", \"node\")\r\n\r\n// And let\u0027s extract the edges and join their vertices with their respective IDs\r\nval edges: RDD[Edge[(String, Long, Long)]] \u003d all\r\n  .join(vertexDf, all(\"Source\") \u003d\u003d\u003d vertexDf(\"node\"))\r\n  .select(all(\"Type\"), all(\"Target\"), all(\"Weight\").alias(\"Weight\"), all(\"Book\").alias(\"Book\"), vertexDf(\"id\").alias(\"idS\"))\r\n  .join(vertexDf, all(\"Target\") \u003d\u003d\u003d vertexDf(\"node\"))\r\n  .select(\"idS\", \"id\", \"Type\", \"Weight\", \"Book\")\r\n  .rdd\r\n  .map { row \u003d\u003e\r\n    Edge(\r\n      row.getAs[Long](\"idS\"),\r\n      row.getAs[Long](\"id\"),\r\n      (row.getAs[String](\"Type\"), row.getAs[Long](\"Weight\"), row.getAs[Long](\"Book\"))\r\n    )\r\n  }\r\n\r\n// And finally\r\nval graph: Graph[Unit, (String, Long, Long)] \u003d Graph.fromEdges(edges, ())\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//AFFICHAGE DES NODES ET RELATIONS\r\n\r\n// Display the first 10 vertices\r\nprintln(\"First 10 Vertices:\")\r\ngraph.vertices.take(10).foreach(println)\r\n\r\n// Display the first 10 edges\r\nprintln(\"First 10 Edges:\")\r\ngraph.edges.take(10).foreach(println)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//MANIPULATION DU GRAPH GRAPHX\r\n\r\n// Nombre de nœuds et d\u0027arêtes\r\nval numVertices \u003d graph.numVertices\r\nval numEdges \u003d graph.numEdges\r\n\r\n// Degré moyen, degré maximum et minimum des nœuds\r\nval avgDegree \u003d graph.degrees.map(_._2).mean()\r\nval maxDegree \u003d graph.degrees.map(_._2).max()\r\nval minDegree \u003d graph.degrees.map(_._2).min()\r\n\r\n// Statistiques des poids et des livres\r\nval weightStats \u003d graph.edges.map(_.attr._2).stats()\r\nval bookStats \u003d graph.edges.map(_.attr._3).stats()\r\n\r\n// Affichage des résultats\r\nprintln(s\"Nombre de nœuds: $numVertices\")\r\nprintln(s\"Nombre d\u0027arêtes: $numEdges\")\r\nprintln(s\"Degré moyen: $avgDegree\")\r\nprintln(s\"Degré maximum: $maxDegree\")\r\nprintln(s\"Degré minimum: $minDegree\")\r\n\r\nprintln(\"Statistiques des poids:\")\r\nprintln(weightStats)\r\n\r\nprintln(\"Statistiques des livres:\")\r\nprintln(bookStats)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//calcule la distribution des degrés des nœuds dans un graphe\r\n\r\nval degreeDistribution \u003d graph.degrees.map{ case (vertexId, degree) \u003d\u003e (degree, vertexId) }.countByValue()\r\n\r\n\r\n\r\n// Display degree distribution with names in descending order\r\nprintln(\"Top 20 Degree Distribution with Names (Descending Order):\")\r\ndegreeDistribution.toSeq.sortBy { case ((degree, vertexId), count) \u003d\u003e -degree }.take(20).foreach { case ((degree, vertexId), count) \u003d\u003e\r\n  val vertexName \u003d vertices.lookup(vertexId).headOption.getOrElse(\"UnknownVertex\")\r\n  println(s\"Vertex $vertexName, Degree $degree: $count nodes\")\r\n}\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//1.PAGE RANK ALGO\r\n\r\nimport org.apache.spark.graphx.lib.PageRank\r\nimport org.apache.spark.sql.types.{StructType, StructField, DoubleType, StringType}\r\n\r\n// Run PageRank\r\nval pageRankGraph \u003d PageRank.run(graph, numIter \u003d 10)\r\n\r\nval schema \u003d StructType(Seq(\r\n  StructField(\"VertexName\", StringType, nullable \u003d false),\r\n  StructField(\"PageRank\", DoubleType, nullable \u003d false)\r\n))\r\n\r\n// Create an empty DataFrame with the defined schema\r\nvar pageRankDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\r\n\r\n// Get the vertices with their PageRank scores\r\nval pageRanks: RDD[(VertexId, Double)] \u003d pageRankGraph.vertices\r\n\r\n// Display the top 10 vertices with their PageRank scores in descending order\r\nprintln(\"Top 10 Vertices with PageRank Scores (Descending Order):\")\r\npageRanks.sortBy(_._2, ascending \u003d false).take(10).foreach { case (vertexId, pageRank) \u003d\u003e\r\n  val vertexName \u003d vertices.lookup(vertexId).headOption.getOrElse(\"UnknownVertex\")\r\n  println(s\"Vertex $vertexName, PageRank: $pageRank\")\r\n  // Append data to the DataFrame\r\n  val newData \u003d Seq((vertexName, pageRank))\r\n  val newRow \u003d spark.createDataFrame(newData).toDF(\"VertexName\", \"PageRank\")\r\n  pageRankDF \u003d pageRankDF.union(newRow)\r\n}\r\n\r\nprintln(\"--------------------------------------------------------\")\r\npageRankDF.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//2.ConnectedComponents Algo\r\n\r\nimport org.apache.spark.graphx.{Graph, VertexId}\r\nimport org.apache.spark.graphx.lib.ConnectedComponents\r\nimport org.apache.spark.sql.{Row, SparkSession}\r\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\r\n\r\n// Assuming you already have \u0027spark\u0027 as your SparkSession\r\n\r\n// Define the schema for connected components DataFrame\r\nval schema \u003d StructType(Seq(\r\n  StructField(\"VertexName\", StringType, nullable \u003d false),\r\n  StructField(\"ConnectedComponent\", LongType, nullable \u003d false)\r\n))\r\n\r\n// Create an empty DataFrame with the defined schema\r\nvar connectedComponentsDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\r\n\r\n// Run ConnectedComponents\r\nval connectedComponentsGraph \u003d ConnectedComponents.run(graph)\r\n\r\n// Get the vertices with their connected component identifiers\r\nval components \u003d connectedComponentsGraph.vertices.collect()\r\n\r\n// Display the connected component of each vertex and create DataFrame\r\nprintln(\"Connected Components:\")\r\ncomponents.foreach { case (vertexId, componentId) \u003d\u003e\r\n  val vertexName \u003d vertices.lookup(vertexId).headOption.getOrElse(\"UnknownVertex\")\r\n  println(s\"Vertex $vertexName, Connected Component: $componentId\")\r\n  // Append data to the DataFrame\r\n  val newData \u003d Seq((vertexName, componentId))\r\n  val newRow \u003d spark.createDataFrame(newData).toDF(\"VertexName\", \"ConnectedComponent\")\r\n  connectedComponentsDF \u003d connectedComponentsDF.union(newRow)\r\n}\r\n\r\nprintln(\"--------------------------------------------------------\")\r\nconnectedComponentsDF.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//3.Label Propagation\r\n\r\n\r\nimport org.apache.spark.graphx.{Graph, VertexId}\r\nimport org.apache.spark.graphx.lib.LabelPropagation\r\n\r\nval schema \u003d StructType(Seq(\r\n  StructField(\"VertexName\", StringType, nullable \u003d false),\r\n  StructField(\"communityLabel\", DoubleType, nullable \u003d false)\r\n))\r\n\r\n// Create an empty DataFrame with the defined schema\r\nvar communityDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\r\n\r\n// Run Label Propagation with a reasonable number of maxSteps\r\nval maxSteps \u003d 10\r\nval labeledGraph \u003d LabelPropagation.run(graph, maxSteps)\r\n\r\n// Get the vertices with their final community labels\r\nval communities \u003d labeledGraph.vertices\r\n\r\n// Display the top 20 vertices with their community labels\r\nprintln(\"Top 20 Vertices with Community Labels:\")\r\ncommunities.sortBy(_._2, ascending \u003d false).take(20).foreach { case (vertexId, communityLabel) \u003d\u003e\r\n  val vertexName \u003d vertices.lookup(vertexId).headOption.getOrElse(\"UnknownVertex\")\r\n  println(s\"Vertex $vertexName, Community Label: $communityLabel\")\r\n  // Append data to the DataFrame\r\n  val newData \u003d Seq((vertexName, communityLabel))\r\n  val newRow \u003d spark.createDataFrame(newData).toDF(\"VertexName\", \"communityLabel\")\r\n  communityDF \u003d communityDF.union(newRow)\r\n}\r\n\r\nprintln(\"--------------------------------------------------------\")\r\ncommunityDF.show()\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//4.TRIANGLECOUNT\r\n\r\n\r\nimport org.apache.spark.graphx._\r\nimport org.apache.spark.graphx.lib.TriangleCount\r\nimport org.apache.spark.sql.{Row, SparkSession}\r\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType}\r\n\r\n// Assuming \u0027graph\u0027 is your original graph\r\n\r\n// Define the schema for the triangle count DataFrame\r\nval schema \u003d StructType(Seq(\r\n  StructField(\"VertexName\", StringType, nullable \u003d false),\r\n  StructField(\"triangleCount\", DoubleType, nullable \u003d false)\r\n))\r\n\r\n// Create an empty DataFrame with the defined schema\r\nvar triangleCountDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\r\n\r\n// Run TriangleCount algorithm on the graph\r\nval triangleCountGraph: Graph[Int, (String, Long, Long)] \u003d TriangleCount.run(graph)\r\n\r\n// Get the vertices with their triangle counts\r\nval triangleCounts \u003d triangleCountGraph.vertices\r\n\r\n// Display the top 20 vertices with their triangle counts\r\nprintln(\"Top 20 Vertices with Triangle Counts:\")\r\ntriangleCounts.sortBy(_._2, ascending \u003d false).take(20).foreach { case (vertexId, triangleCount) \u003d\u003e\r\n  val vertexName \u003d vertices.lookup(vertexId).headOption.getOrElse(\"UnknownVertex\")\r\n  println(s\"Vertex $vertexName, Triangle Count: $triangleCount\")\r\n  // Append data to the DataFrame\r\n  val newData \u003d Seq((vertexName, triangleCount.toDouble))\r\n  val newRow \u003d spark.createDataFrame(newData).toDF(\"VertexName\", \"triangleCount\")\r\n  triangleCountDF \u003d triangleCountDF.union(newRow)\r\n}\r\n\r\nprintln(\"--------------------------------------------------------\")\r\ntriangleCountDF.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//5.STRONGLY CONNECTED\n\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, LongType}\n\n// Assuming \u0027graph\u0027 is your original graph\n\n// Run stronglyConnectedComponents algorithm\nval stronglyConnectedComponents: Graph[VertexId, (String, Long, Long)] \u003d graph.stronglyConnectedComponents(5)\n\n// Define the schema for the strongly connected components DataFrame\nval schema \u003d StructType(Seq(\n  StructField(\"VertexName\", StringType, nullable \u003d false),\n  StructField(\"componentId\", LongType, nullable \u003d false),\n  StructField(\"componentName\", StringType, nullable \u003d false)\n))\n\n// Convert \u0027vertices\u0027 RDD to a DataFrame\nval verticesDF \u003d vertices.toDF(\"VertexId\", \"VertexName\")\n\n// Get the vertices with their component IDs\nval componentVertices \u003d stronglyConnectedComponents.vertices.collect()\n\n// Get the mapping between VertexId and VertexName from the \u0027vertices\u0027 DataFrame\nval vertexMapping \u003d verticesDF.select(\"VertexId\", \"VertexName\").as[(Long, String)].collect.toMap\n\n// Create a sequence of rows for DataFrame creation\nval rows \u003d componentVertices.map { case (vertexId, componentId) \u003d\u003e\n  val vertexName \u003d vertexMapping.getOrElse(vertexId, \"UnknownVertex\")\n  val componentName \u003d vertexMapping.getOrElse(componentId, \"UnknownComponent\")\n  Row(vertexName, componentId, componentName)\n}\n\n// Create the DataFrame once using the sequence of rows\nval sccDF \u003d spark.createDataFrame(spark.sparkContext.parallelize(rows), schema)\n\nprintln(\"Vertices with Strongly Connected Components:\")\nsccDF.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsccDF.coalesce(1).write.csv(\"/zeppelin/notebook/StronglyConnected.csv\")\ntriangleCountDF.coalesce(1).write.csv(\"/zeppelin/notebook/TriangleCount.csv\")\nconnectedComponentsDF.coalesce(1).write.csv(\"/zeppelin/notebook/connectedCompo.csv\")\ncommunityDF.coalesce(1).write.csv(\"/zeppelin/notebook/labelPropa.csv\")\npageRankDF.coalesce(1).write.csv(\"/zeppelin/notebook/pageRankDF.csv\")\n"
    }
  ]
}