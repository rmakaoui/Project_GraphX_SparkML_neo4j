{
  "metadata": {
    "name": "sparkML",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "g%spark.pyspark\n\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"Neo4jConnection\")\\\n    .getOrCreate()\nquery \u003d \"\"\"\nMATCH (source)-[relation]-\u003e(target)\nWHERE (source:Node1 OR target:Node1)\nRETURN source.name as Source, target.name as Target, type(relation) as Type, relation.weight as weight, relation.book as book\n\"\"\"\n\ndata \u003dspark.read.format(\"org.neo4j.spark.DataSource\") \\\n    .option(\"url\", \"bolt://neo4j:7687\") \\\n    .option(\"authentication.type\", \"basic\") \\\n    .option(\"authentication.basic.username\", \"neo4j\") \\\n    .option(\"authentication.basic.password\", \"bitnami1\") \\\n    .option(\"query\", query) \\\n    .option(\"partitions\", \"1\")\\\n    .load()\\"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Vérification des données récupérées depuis Neo4j\ndata.show()\ndata.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit, count, when\nimport pandas as pd\nimport networkx as nx\n\n# Initialize Spark session\nspark \u003d SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"Neo4jConnection\")\\\n    .getOrCreate()\n\n# Define your Neo4j query to fetch data\nquery \u003d \"\"\"\nMATCH (source)-[relation]-\u003e(target)\nWHERE (source:Node1 OR target:Node1)\nRETURN source.name as Source, target.name as Target, type(relation) as Type, relation.weight as weight, relation.book as book\n\"\"\"\n\n# Load data from Neo4j into a Spark DataFrame\ndata \u003d spark.read.format(\"org.neo4j.spark.DataSource\") \\\n    .option(\"url\", \"bolt://neo4j:7687\") \\\n    .option(\"authentication.type\", \"basic\") \\\n    .option(\"authentication.basic.username\", \"neo4j\") \\\n    .option(\"authentication.basic.password\", \"bitnami1\") \\\n    .option(\"query\", query) \\\n    .option(\"partitions\", \"1\")\\\n    .load()\n    \n\n# Add a column for similarity based on the weight of the relationship\ndata \u003d data.withColumn(\"Similarity_Weight\", when(col(\"weight\") \u003e 5, lit(\"High\"))\n                       .when((col(\"weight\") \u003c\u003d 5) \u0026 (col(\"weight\") \u003e 3), lit(\"Medium\"))\n                       .otherwise(lit(\"Low\")))\n\n# Calculate Degree for each node (Source and Target)\ndegrees_source \u003d data.groupBy(\"Source\").agg(count(\"Target\").alias(\"Source_Degree\"))\ndegrees_target \u003d data.groupBy(\"Target\").agg(count(\"Source\").alias(\"Target_Degree\"))\n\n# Join degree with the main DataFrame\ndata \u003d data.join(degrees_source, \"Source\", \"left\").join(degrees_target, \"Target\", \"left\")\n\n\n# Export Spark DataFrame to Pandas\npandas_data \u003d data.toPandas()\n\n# Create a graph using NetworkX from Source and Target columns\ngraph \u003d nx.from_pandas_edgelist(pandas_data, \u0027Source\u0027, \u0027Target\u0027, create_using\u003dnx.Graph())\n\n# Calculate different centrality measures\ndegree_centrality \u003d nx.degree_centrality(graph)\nweighted_degree \u003d dict(graph.degree(weight\u003d\u0027weight\u0027))\neigenvector_centrality \u003d nx.eigenvector_centrality_numpy(graph, weight\u003d\u0027weight\u0027)\npagerank \u003d nx.pagerank(graph, weight\u003d\u0027weight\u0027)\nbetweenness_centrality \u003d nx.betweenness_centrality(graph, weight\u003d\u0027weight\u0027)\n\n# Create Pandas DataFrames for each centrality measure\ndegree_df \u003d pd.DataFrame(list(degree_centrality.items()), columns\u003d[\u0027Node\u0027, \u0027Degree_Centrality\u0027])\nweighted_degree_df \u003d pd.DataFrame(list(weighted_degree.items()), columns\u003d[\u0027Node\u0027, \u0027Weighted_Degree\u0027])\neigenvector_df \u003d pd.DataFrame(list(eigenvector_centrality.items()), columns\u003d[\u0027Node\u0027, \u0027Eigenvector_Centrality\u0027])\npagerank_df \u003d pd.DataFrame(list(pagerank.items()), columns\u003d[\u0027Node\u0027, \u0027PageRank\u0027])\nbetweenness_df \u003d pd.DataFrame(list(betweenness_centrality.items()), columns\u003d[\u0027Node\u0027, \u0027Betweenness_Centrality\u0027])\n\n# Convert Pandas DataFrames to Spark DataFrames\ndegree_spark_df \u003d spark.createDataFrame(degree_df)\nweighted_degree_spark_df \u003d spark.createDataFrame(weighted_degree_df)\neigenvector_spark_df \u003d spark.createDataFrame(eigenvector_df)\npagerank_spark_df \u003d spark.createDataFrame(pagerank_df)\nbetweenness_spark_df \u003d spark.createDataFrame(betweenness_df)\n\n# Join the centrality measures with the original DataFrame\ndata \u003d data.join(degree_spark_df, data.Source \u003d\u003d degree_spark_df.Node, \"left\").drop(\"Node\")\ndata \u003d data.join(weighted_degree_spark_df, data.Source \u003d\u003d weighted_degree_spark_df.Node, \"left\").drop(\"Node\")\ndata \u003d data.join(eigenvector_spark_df, data.Source \u003d\u003d eigenvector_spark_df.Node, \"left\").drop(\"Node\")\ndata \u003d data.join(pagerank_spark_df, data.Source \u003d\u003d pagerank_spark_df.Node, \"left\").drop(\"Node\")\ndata \u003d data.join(betweenness_spark_df, data.Source \u003d\u003d betweenness_spark_df.Node, \"left\").drop(\"Node\")\n\n# Fill null values with 0 in case some nodes don\u0027t have centrality measures\ndata \u003d data.fillna(0)\n\n# Display the final DataFrame with centrality features added\ndata.show()\ndata.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\n# Initialise Spark session\nspark \u003d SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"Predict_Degree_Centrality\")\\\n    .getOrCreate()\n\n# Assume \u0027data\u0027 contient votre DataFrame avec toutes les caractéristiques nécessaires, y compris \u0027Degree_Centrality\u0027\n\n# Liste des colonnes utilisées comme caractéristiques pour la prédiction de Degree_Centrality\nfeature_columns \u003d [\u0027weight\u0027, \u0027book\u0027, \u0027Source_Degree\u0027, \u0027Target_Degree\u0027, \u0027Weighted_Degree\u0027,\n                   \u0027Eigenvector_Centrality\u0027, \u0027PageRank\u0027, \u0027Betweenness_Centrality\u0027]\n\n# Utiliser VectorAssembler pour assembler toutes les colonnes de features en une seule colonne\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\n\n# Créer un objet LinearRegression\nlr \u003d LinearRegression(featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027Degree_Centrality\u0027)\n\n# Créer un pipeline pour assembler les étapes de prétraitement et le modèle\npipeline \u003d Pipeline(stages\u003d[assembler, lr])\n\n# Diviser les données en ensembles d\u0027entraînement et de test (80% pour l\u0027entraînement et 20% pour les tests)\ntrain_data, test_data \u003d data.randomSplit([0.8, 0.2], seed\u003d12345)\n\n# Entraîner le modèle sur les données d\u0027entraînement\nmodel \u003d pipeline.fit(train_data)\n\n# Faire des prédictions sur les données de test\npredictions \u003d model.transform(test_data)\n\n# Afficher les prédictions et les valeurs réelles\npredictions.select(\u0027Degree_Centrality\u0027, \u0027prediction\u0027, *feature_columns).show()\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Calculer la MSE\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"Degree_Centrality\", predictionCol\u003d\"prediction\", metricName\u003d\"mse\")\nmse \u003d evaluator.evaluate(predictions)\nprint(\"Mean Squared Error (MSE) on test data \u003d {:.4f}\".format(mse))\n\n# Calculer la RMSE\nrmse \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\nprint(\"Root Mean Squared Error (RMSE) on test data \u003d {:.4f}\".format(rmse))\n\n# Calculer R² (Coefficient de détermination)\nr2 \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\nprint(\"R-squared (R²) on test data \u003d {:.4f}\".format(r2))\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Créer un VectorAssembler pour assembler les colonnes de features\nfeature_columns \u003d [\u0027Source_Degree\u0027, \u0027Target_Degree\u0027, \u0027Degree_Centrality\u0027, \u0027Weighted_Degree\u0027,\n                   \u0027Eigenvector_Centrality\u0027, \u0027Betweenness_Centrality\u0027, \u0027weight\u0027]\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\n\n# Créer un modèle DecisionTreeRegressor\ndt \u003d DecisionTreeRegressor(featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027PageRank\u0027)\n\n# Créer un pipeline\npipeline \u003d Pipeline(stages\u003d[assembler, dt])\n\n# Diviser les données en ensembles d\u0027entraînement et de test\ntrain_data, test_data \u003d data.randomSplit([0.8, 0.2], seed\u003d12345)\n\n# Entraîner le modèle\nmodel \u003d pipeline.fit(train_data)\n\n# Faire des prédictions sur les données de test\npredictions \u003d model.transform(test_data)\n\n# Afficher les prédictions et les valeurs réelles\npredictions.select(\u0027PageRank\u0027, \u0027prediction\u0027, *feature_columns).show()\n\n# Calculer des métriques d\u0027évaluation pour le modèle\n\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"PageRank\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\nmse \u003d evaluator.evaluate(predictions)\nprint(\"Mean Squared Error (MSE) on test data \u003d {:.4f}\".format(mse))\nrmse \u003d evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data \u003d {:.4f}\".format(rmse))\n\nr2 \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\nprint(\"R-squared (R²) on test data \u003d {:.4f}\".format(r2))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql import SparkSession\n\n# Initialise Spark session\nspark \u003d SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"Betweenness_Prediction_RF\")\\\n    .getOrCreate()\n\n# Assume \u0027data\u0027 contains your prepared DataFrame with all the features and target variable \u0027Betweenness_Centrality\u0027\n\n# Utiliser VectorAssembler pour assembler toutes les colonnes de features en une seule colonne\nfeature_columns \u003d [\u0027Source_Degree\u0027, \u0027Target_Degree\u0027, \u0027Degree_Centrality\u0027, \u0027Weighted_Degree\u0027,\n                   \u0027Eigenvector_Centrality\u0027, \u0027PageRank\u0027, \u0027weight\u0027]  # Ajoutez \u0027weight\u0027 comme feature\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\n\n# Créer un objet RandomForestRegressor\nrf \u003d RandomForestRegressor(featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027Betweenness_Centrality\u0027)\n\n# Créer un pipeline pour assembler les étapes de prétraitement et le modèle\npipeline \u003d Pipeline(stages\u003d[assembler, rf])\n\n# Diviser les données en ensembles d\u0027entraînement et de test (80% pour l\u0027entraînement et 20% pour les tests)\ntrain_data, test_data \u003d data.randomSplit([0.8, 0.2], seed\u003d12345)\n\n# Entraîner le modèle sur les données d\u0027entraînement\nmodel \u003d pipeline.fit(train_data)\n\n# Faire des prédictions sur les données de test\npredictions \u003d model.transform(test_data)\n\n# Afficher les prédictions et les valeurs réelles\npredictions.select(\u0027Betweenness_Centrality\u0027, \u0027prediction\u0027, *feature_columns).show()\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Calculer la MSE\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"Betweenness_Centrality\", predictionCol\u003d\"prediction\", metricName\u003d\"mse\")\nmse \u003d evaluator.evaluate(predictions)\nprint(\"Mean Squared Error (MSE) on test data \u003d {:.4f}\".format(mse))\n\n# Calculer la RMSE\nrmse \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\nprint(\"Root Mean Squared Error (RMSE) on test data \u003d {:.4f}\".format(rmse))\n\n# Calculer R² (Coefficient de détermination)\nr2 \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\nprint(\"R-squared (R²) on test data \u003d {:.4f}\".format(r2))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\n# Initialise Spark session\nspark \u003d SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"Predict_Weighted_Degree_RF\")\\\n    .getOrCreate()\n\n# Assume \u0027data\u0027 contains your prepared DataFrame with all the features and target variable \u0027Weighted_Degree\u0027\n\n# Utiliser VectorAssembler pour assembler toutes les colonnes de features en une seule colonne\nfeature_columns \u003d [\u0027Source_Degree\u0027, \u0027Target_Degree\u0027, \u0027Degree_Centrality\u0027, \u0027weight\u0027, \u0027Eigenvector_Centrality\u0027, \u0027PageRank\u0027, \u0027Betweenness_Centrality\u0027]\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\n\n# Créer un objet RandomForestRegressor\nrf \u003d RandomForestRegressor(featuresCol\u003d\u0027features\u0027, labelCol\u003d\u0027Weighted_Degree\u0027)\n\n# Créer un pipeline pour assembler les étapes de prétraitement et le modèle\npipeline \u003d Pipeline(stages\u003d[assembler, rf])\n\n# Diviser les données en ensembles d\u0027entraînement et de test (80% pour l\u0027entraînement et 20% pour les tests)\ntrain_data, test_data \u003d data.randomSplit([0.8, 0.2], seed\u003d12345)\n\n# Entraîner le modèle sur les données d\u0027entraînement\nmodel \u003d pipeline.fit(train_data)\n\n# Faire des prédictions sur les données de test\npredictions \u003d model.transform(test_data)\n\n# Afficher les prédictions et les valeurs réelles\npredictions.select(\u0027Weighted_Degree\u0027, \u0027prediction\u0027, *feature_columns).show()\n\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Calculer la MSE\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"Weighted_Degree\", predictionCol\u003d\"prediction\", metricName\u003d\"mse\")\nmse \u003d evaluator.evaluate(predictions)\nprint(\"Mean Squared Error (MSE) on test data \u003d {:.4f}\".format(mse))\n\n# Calculer la RMSE\nrmse \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\nprint(\"Root Mean Squared Error (RMSE) on test data \u003d {:.4f}\".format(rmse))\n\n# Calculer R² (Coefficient de détermination)\nr2 \u003d evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\nprint(\"R-squared (R²) on test data \u003d {:.4f}\".format(r2))\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.ml.clustering import KMeans\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.sql.functions import col\r\n\r\n# Initialise Spark session\r\nspark \u003d SparkSession.builder.master(\"local[1]\").appName(\"Weight_Clustering\").getOrCreate()\r\n\r\n# Convertir la colonne \"weight\" en type numérique (double)\r\ndata \u003d data.withColumn(\"weight\", col(\"weight\").cast(\"double\"))\r\n\r\n# Supprimer les lignes avec des valeurs nulles dans la colonne \u0027weight\u0027\r\ndata \u003d data.dropna(subset\u003d[\"weight\"])\r\n\r\n# Assembler les caractéristiques en un vecteur, en traitant les valeurs nulles\r\nassembler \u003d VectorAssembler(inputCols\u003d[\"weight\"], outputCol\u003d\"features\", handleInvalid\u003d\"keep\")\r\nassembled_df \u003d assembler.transform(data)\r\n\r\n# Créer le modèle K-Means avec k\u003d3 clusters\r\nkmeans \u003d KMeans(featuresCol\u003d\"features\", k\u003d3)\r\n\r\n# Entraîner le modèle K-Means\r\nkmeans_model \u003d kmeans.fit(assembled_df)\r\n\r\n# Faire des prédictions sur les données\r\npredictions \u003d kmeans_model.transform(assembled_df)\r\n\r\n# Afficher les données d\u0027origine et les prédictions du K-Means\r\nresult \u003d predictions.select(\"Source\", \"Target\", \"weight\", \"prediction\")\r\nresult.show(100)\r\n\r\nresult.coalesce(1).write.csv(\u0027/zeppelin/notebook/result4.csv\u0027, header\u003dTrue)\r\n\r\n\r\n# Enregistrer les résultats au format CSV\r\n\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n# Load the saved CSV file into a Pandas DataFrame\r\nresult_pandas \u003d pd.read_csv(\u0027/zeppelin/notebook/result3.csv/part-00000-fbb41e2a-e96e-4507-aeea-28e1a1717cae-c000.csv\u0027)\r\n\r\n# Extract relevant columns for visualization\r\nx \u003d result_pandas[\u0027weight\u0027]\r\ny \u003d result_pandas[\u0027prediction\u0027]\r\n\r\n# Plotting the scatter plot\r\nplt.figure(figsize\u003d(8, 6))\r\nplt.scatter(x, y, c\u003dy, cmap\u003d\u0027viridis\u0027, edgecolors\u003d\u0027k\u0027)\r\nplt.title(\u0027K-Means Clustering\u0027)\r\nplt.xlabel(\u0027Weight\u0027)\r\nplt.ylabel(\u0027Cluster\u0027)\r\nplt.colorbar(label\u003d\u0027Cluster\u0027)\r\nplt.grid(True)\r\nplt.show()\r\nz.show(plt)\r\n\r\n\r\n\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}